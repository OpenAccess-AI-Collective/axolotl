{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e02b2ab-d65d-49e0-843c-8294dac77ce0",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from axolotl.utils.dict import DefaultDict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "124440aa-9c3b-4b50-809b-7581641ba159",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "cfg = DefaultDict({\n",
    "    \"base_model\": \"TinyLlama/TinyLlama-1.1B-intermediate-step-1431k-3T\",\n",
    "    \"load_in_8bit\": True,\n",
    "    \"datasets\": [{\"path\": \"mhenrichsen/alpaca_2k_test\", \"type\": \"alpaca\"}],\n",
    "    \"val_set_size\": 0.1,\n",
    "    \"output_dir\": \"./lora-out\",\n",
    "    \"sequence_len\": 2048,\n",
    "    \"sample_packing\": True,\n",
    "    \"pad_to_sequence_len\": True,\n",
    "    \"adapter\": \"lora\",\n",
    "    \"lora_r\": 32,\n",
    "    \"lora_alpha\": 16,\n",
    "    \"lora_dropout\": 0.1,\n",
    "    \"lora_target_linear\": true,\n",
    "    \"gradient_accumulation_steps\": 4,\n",
    "    \"micro_batch_size\": 2,\n",
    "    \"num_epochs\": 1,\n",
    "    \"optimizer\": \"adamw_bnb_8bit\",\n",
    "    \"lr_scheduler\": \"cosine\",\n",
    "    \"learning_rate\": 0.0002,\n",
    "    \"bf16\": \"auto\",\n",
    "    \"gradient_checkpointing\": True,\n",
    "    \"logging_steps\": 1,\n",
    "    \"flash_attention\": True,\n",
    "    \"warmup_steps\": 10,\n",
    "    \"evals_per_epoch\": 4,\n",
    "    \"saves_per_epoch\": 1,\n",
    "    \"weight_decay\": 0.0,\n",
    "})\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": ""
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
