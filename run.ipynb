{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1da7097-f38f-4e5b-9f34-d7e2e8d25034",
   "metadata": {},
   "outputs": [],
   "source": [
    "!git clone https://apttechsols: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@github.com/code-x-0018/axolotl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b444f0b8-baa1-45d7-85b2-1e86a259ad2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "cd axolotl/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2edabfa7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/workspace/axolotl'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pwd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdd6d763",
   "metadata": {},
   "source": [
    "**Setup**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a08676f3-79a3-4782-81da-4b433d0f1f72",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (23.2)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip3 install packaging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfcb780c-a73f-445a-96b7-fc818c9f8506",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip3 install -e '.[flash-attn,deepspeed]'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1c6b884",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install torch torchvision torchaudio transformers huggingface_hub deepspeed accelerate peft bitsandbytes langchain wandb scipy datasets openai tensorboard sentencepiece xformers boto3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "83ee8b54",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from huggingface_hub import HfFolder, snapshot_download\n",
    "import os\n",
    "import wandb\n",
    "import boto3\n",
    "from transformers import (\n",
    "    AutoModelForCausalLM,\n",
    "    AutoTokenizer,\n",
    "    BitsAndBytesConfig,\n",
    "    HfArgumentParser,\n",
    "    TrainingArguments,\n",
    "    pipeline,\n",
    "    logging,\n",
    "    TextStreamer,\n",
    "    StoppingCriteria,\n",
    "    StoppingCriteriaList\n",
    ")\n",
    "from langchain import HuggingFacePipeline\n",
    "from langchain import PromptTemplate, LLMChain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "55047159",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mdeveloper-team018\u001b[0m (\u001b[33mneural-network-018\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n"
     ]
    }
   ],
   "source": [
    "hf_token = \"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\"\n",
    "hf_folder = HfFolder()\n",
    "hf_folder.save_token(hf_token)\n",
    "\n",
    "wandb.login(key=\"3dcf50eff227fa779cbda8281597f06304583769\")\n",
    "\n",
    "available_gpus = torch.cuda.device_count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60220cfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# rm -r checkpoints"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f74d65e",
   "metadata": {},
   "source": [
    "**Train**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0d4cdeb-c1b4-4f57-8734-34b8a3e68c30",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CUDA_VISIBLE_DEVICES=\"\"\n",
    "# \n",
    "# !python3 ./scripts/finetune.py ./configs/models/01-ai/config.yml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4d72e8e-f550-47d4-bbe3-e1c5d545c887",
   "metadata": {},
   "outputs": [],
   "source": [
    "!accelerate launch -m axolotl.cli.train examples/01-ai/config-34b.yml --num_processes=8 --num_machines=1 --deepspeed deepspeed/zero3.json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6eb52baf-0917-450f-9411-be54db66662e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CUDA_VISIBLE_DEVICES=\"\"\n",
    "# \n",
    "# !accelerate launch -m axolotl.cli.train examples/mistral/config.yml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cf1f65a-ac63-438b-87f8-7b858aac320a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "b7fa33d6",
   "metadata": {},
   "source": [
    "**Inferencing**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aba7b023",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_model_path = \"./checkpoints\"\n",
    "\n",
    "# Reload model in FP16 and merge it with LoRA weights\n",
    "model_new = AutoModelForCausalLM.from_pretrained(\n",
    "    new_model_path,\n",
    "    device_map=\"auto\",\n",
    "    trust_remote_code=True\n",
    ")\n",
    "\n",
    "# Reload tokenizer to save it\n",
    "tokenizer_new = AutoTokenizer.from_pretrained(new_model_path, trust_remote_code=True)\n",
    "# tokenizer.add_special_tokens({'pad_token': '[PAD]'})\n",
    "# tokenizer.pad_token = tokenizer.eos_token\n",
    "# tokenizer.padding_side = \"right\"\n",
    "print(\"Model & tokenizer loaded\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffa3c8b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomTokenStoppingCriteria:\n",
    "    \"\"\"\n",
    "    Stop when a custom token is encountered.\n",
    "    \"\"\"\n",
    "    def __init__(self, max_token):\n",
    "        self.max_token = max_token\n",
    "\n",
    "    def __call__(self, inputs, scores):\n",
    "        if self.max_token in inputs[0].tolist():\n",
    "            return True\n",
    "        return False\n",
    "\n",
    "end_of_document_token = tokenizer_new.encode(tokenizer_new.eos_token)[0]\n",
    "custom_stop_token = tokenizer_new.encode(\"</s>\")[0]\n",
    "\n",
    "stopping_criteria = StoppingCriteriaList([\n",
    "    CustomTokenStoppingCriteria(end_of_document_token),\n",
    "    CustomTokenStoppingCriteria(custom_stop_token)\n",
    "])\n",
    "\n",
    "streamer = TextStreamer(tokenizer_new, skip_prompt=True)\n",
    "stream_pipeline = pipeline(\n",
    "    \"text-generation\",\n",
    "    model=model_new,\n",
    "    tokenizer=tokenizer_new,\n",
    "    max_new_tokens=4096,\n",
    "    temperature=0.6,\n",
    "    top_p=0.9,\n",
    "    do_sample=True,\n",
    "    top_k=50,\n",
    "    repetition_penalty=1.15,\n",
    "    batch_size=8,\n",
    "    streamer=streamer,\n",
    ")\n",
    "stream_llm = HuggingFacePipeline(pipeline=stream_pipeline)\n",
    "stream_template = \"\"\"<|im_system|>Act as an helpful assistant. Whenever a user poses a question or presents an issue, respond with clear, accurate, and comprehensive solutions or explanations tailored to address the query. Prioritize user clarity and the relevancy of your response.<|end_system|>\n",
    "<|im_category|>math, coding<|end_category|>\n",
    "<|im_user|>{query}<|end_user|><|im_assistant|>\n",
    "\"\"\"\n",
    "stream_prompt = PromptTemplate(template=stream_template, input_variables=[\"query\"])\n",
    "stream_llm_chain = LLMChain(prompt=stream_prompt, llm=stream_llm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c07d6b56",
   "metadata": {},
   "outputs": [],
   "source": [
    "stream_llm_chain.run(\"Who is onwer of twitter?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc4707dc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "4058b45f",
   "metadata": {},
   "source": [
    "**Upload model on HF**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e4da7a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.system(f'huggingface-cli login')\n",
    "#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\n",
    "\n",
    "model_new.push_to_hub(new_model_id, use_temp_dir=False)\n",
    "tokenizer_new.push_to_hub(new_model_id, use_temp_dir=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d849804b",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install huggingface_hub\n",
    "You already have it if you installed transformers or datasets\n",
    "\n",
    "huggingface-cli login\n",
    "Log in using a token from huggingface.co/settings/tokens\n",
    "Create a model or dataset repo from the CLI if needed\n",
    "huggingface-cli repo create repo_name --type {model, dataset, space}\n",
    "\n",
    "git lfs install\n",
    "git clone https://huggingface.co/username/repo_name\n",
    "\n",
    "git add .\n",
    "git commit -m \"commit from $USER\"\n",
    "git push"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00ec1171",
   "metadata": {},
   "source": [
    "**Upload On AWS S3**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0c3c3a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "boto3.setup_default_session(aws_access_key_id='AKIA5M5ZEZRL5WD2Z6XV', aws_secret_access_key='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', region_name='ap-south-1')\n",
    "s3 = boto3.client('s3')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e246901",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Directory path\n",
    "main_model = \"mistral\"\n",
    "new_model_name = \"apt-chat-mistral-7b-v1\"\n",
    "s3_folder = f\"models/apt-chat/hf/{main_model}/finetune/{new_model_name}\"\n",
    "\n",
    "local_directory = \"./checkpoints/\"\n",
    "s3_bucket = \"aptai\"\n",
    "\n",
    "# Recursively walk through the local directory\n",
    "for root, dirs, files in os.walk(local_directory):\n",
    "    for file in files:\n",
    "        # Construct the full local path\n",
    "        local_path = os.path.join(root, file)\n",
    "        \n",
    "        # Construct the full S3 path\n",
    "        relative_path = os.path.relpath(local_path, local_directory)\n",
    "        s3_path = os.path.join(s3_folder, relative_path)\n",
    "        \n",
    "        # Upload the file\n",
    "        print(f'Uploading {local_path} to {s3_path}')\n",
    "        s3.upload_file(local_path, s3_bucket, s3_path)\n",
    "\n",
    "print('Upload complete')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
